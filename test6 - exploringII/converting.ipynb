{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea is to \n",
    "# 1) extract on and offset\n",
    "# 2) do the same feature extraction method as training\n",
    "# 3) load the model and predict notes\n",
    "# 4) use the onset offset info together convert to midi\n",
    "# 5) convert to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "audio_path = \"../data/wav/en001b.wav\"\n",
    "audio_data, sr = librosa.load(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract onset and offset timings\n",
    "onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, backtrack=True)\n",
    "onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "offset_times = librosa.frames_to_time(onset_frames[1:], sr=sr)  # Assuming offset is the next onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tempo\n",
    "tempo, _ = librosa.beat.beat_track(y=audio_data, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(onset_times)<len(offset_times):\n",
    "    offset_times = offset_times[:len(onset_times)]\n",
    "else:\n",
    "    onset_times = onset_times[:len(offset_times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_data, onset, offset):\n",
    "    #trim\n",
    "    y_trimmed, _ = librosa.effects.trim(audio_data)\n",
    "    # Extract the audio segment\n",
    "    segment = y_trimmed[int(onset * sr):int(offset * sr)]\n",
    "    # Extract features (e.g., MFCCs)\n",
    "    features = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n",
    "    return features.T  # Transpose to have shape (n_frames, n_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\OneDrive - Singapore Management University\\Desktop\\data mining\\project\\test\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1536\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\OneDrive - Singapore Management University\\Desktop\\data mining\\project\\test\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for i in range(len(onset_times)):\n",
    "    segment_features = extract_features(audio_data, onset_times[i], offset_times[i])\n",
    "    features.append(segment_features)\n",
    "\n",
    "features = np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2763, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.shape\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "features_scaled = scaler.transform(features)\n",
    "f_reshaped = features_scaled.reshape(features_scaled.shape[0],1,features_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_21720\\506932380.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred = [int(i) for i in pred]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./model.h5')\n",
    "\n",
    "pred = model.predict(f_reshaped)\n",
    "# pred = model.predict(features_scaled)\n",
    "pred = [int(i) for i in pred]\n",
    "# pred[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../output/'\n",
    "\n",
    "# Function to convert pitches to MIDI notes\n",
    "def pitches_to_midi(pitches, output_file_path, tempo):\n",
    "    # Create a PrettyMIDI object\n",
    "    midi_data = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "    piano = pretty_midi.Instrument(program=piano_program)\n",
    "\n",
    "    # Add notes to the piano instrument\n",
    "    for i, pitch in enumerate(pitches):\n",
    "        if pitch != 0:  # Skip rests (assuming pitch 0 represents a rest)\n",
    "            note = pretty_midi.Note(velocity=127, pitch=int(pitch), start=i, end=i+1)  # Assuming each pitch lasts for one unit of time\n",
    "            piano.notes.append(note)\n",
    "\n",
    "    # Add the piano instrument to the MIDI data\n",
    "    midi_data.instruments.append(piano)\n",
    "\n",
    "    # Write the MIDI data to a file\n",
    "    midi_data.write(output_file_path+'output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches_to_midi(pred, output_path, tempo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.T.shape\n",
    "# # Reshape MFCCs to match the model input shape\n",
    "# # mfccs_reshaped = np.expand_dims(features.T, axis=0)\n",
    "\n",
    "# scaler = joblib.load('scaler.pkl')\n",
    "# mfccs_scaled = scaler.transform(features.T)\n",
    "# # mfccs_scaled.shape\n",
    "# mfccs_reshaped = np.expand_dims(features.T, axis=0)\n",
    "# # mfccs_reshaped.shape\n",
    "# X_actual_reshaped = mfccs_reshaped.reshape(mfccs_reshaped.shape[1],1,mfccs_reshaped.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('./model.h5')\n",
    "\n",
    "# pred = model.predict(X_actual_reshaped)\n",
    "# pred = [int(i) for i in pred]\n",
    "# pred[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
