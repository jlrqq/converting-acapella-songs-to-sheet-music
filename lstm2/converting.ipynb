{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idea is to \n",
    "# 1) extract on and offset\n",
    "# 2) do the same feature extraction method as training\n",
    "# 3) load the model and predict notes\n",
    "# 4) use the onset offset info together convert to midi\n",
    "# 5) convert to sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio file\n",
    "audio_path = \"../data/wav/en001b.wav\"\n",
    "audio_data, sr = librosa.load(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract onset and offset timings\n",
    "onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, backtrack=True)\n",
    "onset_times = librosa.frames_to_time(onset_frames, sr=sr)\n",
    "offset_times = librosa.frames_to_time(onset_frames[1:], sr=sr)  # Assuming offset is the next onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tempo\n",
    "tempo, _ = librosa.beat.beat_track(y=audio_data, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(onset_times)<len(offset_times):\n",
    "    offset_times = offset_times[:len(onset_times)]\n",
    "else:\n",
    "    onset_times = onset_times[:len(offset_times)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_data, onset, offset):\n",
    "    #trim\n",
    "    y_trimmed, _ = librosa.effects.trim(audio_data)\n",
    "    # Extract the audio segment\n",
    "    segment = y_trimmed[int(onset * sr):int(offset * sr)]\n",
    "    # Extract features (e.g., MFCCs)\n",
    "    features = librosa.feature.mfcc(y=segment, sr=sr, n_mfcc=13)\n",
    "    return features.T  # Transpose to have shape (n_frames, n_mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\OneDrive - Singapore Management University\\Desktop\\data mining\\project\\test\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=1536\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\OneDrive - Singapore Management University\\Desktop\\data mining\\project\\test\\.venv\\lib\\site-packages\\librosa\\core\\spectrum.py:257: UserWarning: n_fft=2048 is too large for input signal of length=0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for i in range(len(onset_times)):\n",
    "    segment_features = extract_features(audio_data, onset_times[i], offset_times[i])\n",
    "    features.append(segment_features)\n",
    "\n",
    "features = np.vstack(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2763, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features.shape\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "features_scaled = scaler.transform(features)\n",
    "f_reshaped = features_scaled.reshape(features_scaled.shape[0],1,features_scaled.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24388\\506932380.py:5: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred = [int(i) for i in pred]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./model.h5')\n",
    "\n",
    "pred = model.predict(f_reshaped)\n",
    "# pred = model.predict(features_scaled)\n",
    "pred = [int(i) for i in pred]\n",
    "# pred[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_midi(predicted_pitches, onset_times, offset_times, tempo=100):\n",
    "    # Create a PrettyMIDI object\n",
    "    midi_data = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "\n",
    "    # Create an Instrument instance for the piano\n",
    "    piano_program = pretty_midi.instrument_name_to_program('Acoustic Grand Piano')\n",
    "    piano = pretty_midi.Instrument(program=piano_program)\n",
    "\n",
    "    # Convert predicted pitches to MIDI notes\n",
    "    for pitch, onset, offset in zip(predicted_pitches, onset_times, offset_times):\n",
    "        note = pretty_midi.Note(\n",
    "            velocity=100, pitch=int(pitch), start=onset, end=offset\n",
    "        )\n",
    "        piano.notes.append(note)\n",
    "\n",
    "    # Add the piano instrument to the PrettyMIDI object\n",
    "    midi_data.instruments.append(piano)\n",
    "\n",
    "    # Write the MIDI data to a file\n",
    "    midi_data.write('output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_midi(pred, onset_times, offset_times, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../output/'\n",
    "\n",
    "# Function to convert pitches to MIDI notes\n",
    "def pitches_to_midi(onset_times, offset_times, pitches, output_file_path, tempo=100):\n",
    "    midi = MidiFile()\n",
    "    track = MidiTrack()\n",
    "    midi.tracks.append(track)\n",
    "\n",
    "    ticks_per_beat = 480 #standard MIDI ticks per beat #midi.ticks_per_beat\n",
    "\n",
    "    track.append(MetaMessage('set_tempo', tempo=tempo))\n",
    "\n",
    "    # Assign MIDI note numbers to pitches\n",
    "    min_pitch = min(pred)  # MIDI note number for C4\n",
    "    max_pitch = max(pred)  # MIDI note number for C5\n",
    "    pitch_range = max_pitch - min_pitch\n",
    "\n",
    "    interpolated_pitches = []\n",
    "    for i in range(len(onset_times)-1):\n",
    "        start_time = onset_times[i]\n",
    "        end_time = offset_times[i]\n",
    "        duration = end_time - start_time\n",
    "        num_steps = int(duration * ticks_per_beat)\n",
    "\n",
    "        if num_steps == 0:\n",
    "            continue\n",
    "\n",
    "        start_pitch = pitches[i]\n",
    "        end_pitch = pitches[i+1]\n",
    "        pitch_diff = end_pitch - start_pitch\n",
    "        pitch_step = pitch_diff / num_steps\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            interpolated_pitch = start_pitch + step * pitch_step\n",
    "            interpolated_pitches.append(interpolated_pitch)\n",
    "\n",
    "    current = 0\n",
    "    for pitch in interpolated_pitches:\n",
    "        # Calculate the MIDI note number\n",
    "        predicted_pitch = min_pitch + int((pitch * pitch_range) % pitch_range)\n",
    "\n",
    "        # Create a note-on message\n",
    "        track.append(Message('note_on', note=predicted_pitch, velocity=100, time=current))\n",
    "\n",
    "        # Create a note-off message (assuming a fixed duration for each note, adjust as needed)\n",
    "        track.append(Message('note_off', note=predicted_pitch, velocity=100, time=current + ticks_per_beat))\n",
    "\n",
    "        current += ticks_per_beat\n",
    "\n",
    "\n",
    "\n",
    "    # Save the MIDI file\n",
    "    midi.save(output_file_path + 'output.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitches_to_midi(onset_times, offset_times, pred, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
